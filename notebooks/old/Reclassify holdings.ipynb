{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# Auxiliary functions\n",
    "def id_col_clean(col):\n",
    "    new_col = pd.Series(col.astype(str).str.strip().str.upper())\n",
    "# remove '0' and other non valid values\n",
    "    new_col = new_col.apply(\n",
    "        lambda x: None if (x == '0') | (x == 'NAN') | (x == 'NONE') | (x == '') else x\n",
    "    )\n",
    "    return new_col\n",
    "\n",
    "\n",
    "def any_heb_char(s):\n",
    "    s = str(s)\n",
    "    # df[\"has_hebrew_char\"] = df[string_column].map(lambda s: any_heb_char(s))\n",
    "    return any(\"\\u0590\" <= c <= \"\\u05EA\" for c in s)\n",
    "\n",
    "\n",
    "def clean_ticker(s):\n",
    "    s = str(s)\n",
    "    # remove trailing digits\n",
    "    without_trailing_digits = s.rstrip(string.digits)\n",
    "    # remove everything after a dot or a space from tickers.\n",
    "    return re.sub(r'[\\\\/%\\s]+$', '', without_trailing_digits).partition('.')[0].partition(' ')[0]\n",
    "\n",
    "\n",
    "def clean_instrument_from_ticker(df, instrument_col, ticker_col):\n",
    "    # remove ticker and everything that follows from instrument name if it appears in the 3rd word or later\n",
    "    # e.g. AKER BP ASA AKERBP 4 3/4 06/15/24 --> AKER BP, AIB GROUP PLC AIB 5 1/4 PERP --> AIB GROUP PLC\n",
    "    instruments = df\n",
    "    instruments[\"instrument_word_list\"] = instruments[instrument_col].str.split()\n",
    "    instruments[\"ticker_first\"] = instruments[ticker_col].str.split().str.get(0)\n",
    "    instruments[\"ticker_in_name\"] = instruments.apply(\n",
    "        lambda row: row[\"instrument_word_list\"][1:].index(row[\"ticker_first\"]) + 1\n",
    "        if row[\"ticker_first\"] in row[\"instrument_word_list\"][1:]\n",
    "        else np.nan,\n",
    "        axis=1)\n",
    "    instruments[\"company_name_cut_ticker\"] = instruments.apply(\n",
    "        lambda row: ' '.join(row[\"instrument_word_list\"][:int(row[\"ticker_in_name\"])]) if row[\"ticker_in_name\"] > 1\n",
    "        else row[instrument_col],\n",
    "        axis=1)\n",
    "    return instruments.drop([\"instrument_word_list\", \"ticker_first\", \"ticker_in_name\"], axis=1)\n",
    "\n",
    "\n",
    "def clean_company(s):\n",
    "    s = str(s)\n",
    "    return re.sub(r'[\\\\/.%\\s\\d]+$', '', s)\n",
    "\n",
    "\n",
    "def company_names_match_score(row, holdings_company_col, fff_company_col, min_len=3):\n",
    "    holdings_company_name = str(row[holdings_company_col]).strip().lower()\n",
    "    fff_company_name = str(row[fff_company_col]).strip().lower()\n",
    "    if (holdings_company_name == 'nan') | (fff_company_name == 'nan'):\n",
    "        return np.nan\n",
    "    if (len(holdings_company_name) >= min_len) & (len(fff_company_name) >= min_len):\n",
    "        return fuzz.partial_ratio(holdings_company_name, fff_company_name)\n",
    "\n",
    "\n",
    "def get_common_words_in_company_name(holdings, fff, holdings_company_col, fff_company_col):\n",
    "    # returns a list of common words, to be disregarded when matching by company names\n",
    "    # print(fff[fff_company_col].str.split(expand=True).stack().value_counts().head(30))\n",
    "    # print(holdings[holdings_company_col].str.split(expand=True).stack().value_counts().head(30))\n",
    "    common_words_company_name = ['LTD', 'INC', 'CORP', 'CO', 'GROUP', 'PLC', 'HOLDINGS', '&', 'FLOAT', 'אגח']\n",
    "    return common_words_company_name\n",
    "\n",
    "\n",
    "def remove_common_words(l, common):\n",
    "    res = []\n",
    "    for x in l:\n",
    "        x = str(x)\n",
    "        new = ' '.join([word for word in x.split() if word not in common])\n",
    "        res.append(new)\n",
    "    return res\n",
    "\n",
    "\n",
    "def find_isin_col(df):\n",
    "    '''\n",
    "    Automatically identify columns with ISINs\n",
    "    :param df: DataFrame\n",
    "    :return: isin_col: string\n",
    "    '''\n",
    "    isin_pattern = r\"^[A-Z]{2}([A-Z0-9]){9}[0-9]$\"\n",
    "    max_isin_cnt = 0\n",
    "    for col in df:\n",
    "        isin_cnt = sum(df[col].astype(str).str.contains(isin_pattern, na=False))\n",
    "        if isin_cnt > max_isin_cnt:\n",
    "            isin_col = col\n",
    "            max_isin_cnt = isin_cnt\n",
    "\n",
    "    if max_isin_cnt > 0:\n",
    "        print(\"\\nHolding file ISIN col is: \" + isin_col)\n",
    "        print(\"number of ISINs: {} out of {} rows\".format(max_isin_cnt, df.shape[0]))\n",
    "        return isin_col\n",
    "    else:\n",
    "        print(\"\\nERROR: no ISINs in holdings file\")\n",
    "\n",
    "\n",
    "def find_il_corp_num_col(df):\n",
    "    '''\n",
    "    Automatically identify columns with Israeli Corp Numbers (מספר תאגיד)\n",
    "    :param df:\n",
    "    :return:\n",
    "    '''\n",
    "    pattern = r\"^5([0-9]){8}$\"\n",
    "    max_pattern_cnt = 0\n",
    "    for col in df:\n",
    "        pattern_cnt = sum(df[col].astype(str).str.contains(pattern, na=False))\n",
    "        if pattern_cnt > max_pattern_cnt:\n",
    "            max_col = col\n",
    "            max_pattern_cnt = pattern_cnt\n",
    "\n",
    "    if max_pattern_cnt > 0:\n",
    "        print(\"\\nHolding file Israel Corp col is: \" + max_col)\n",
    "        print(\"number of Israel Corp Numbers: {} out of {} rows\".format(max_pattern_cnt, df.shape[0]))\n",
    "        return max_col\n",
    "    else:\n",
    "        print(\"\\nERROR: no Israel Corp Numbers in holdings file\")\n",
    "\n",
    "\n",
    "def is_tlv(df, isin_col):\n",
    "    return df[isin_col].str.isdigit().fillna(False)\n",
    "\n",
    "\n",
    "# Fossil Free Funds list functions\n",
    "def fetch_latest_fff_list():\n",
    "    # fetch newest file from Fossil Free Funds\n",
    "    # returns Dataframe read from excel file\n",
    "    site = \"https://fossilfreefunds.org/how-it-works\"\n",
    "    hdr = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Connection': 'keep-alive'}\n",
    "    req = urllib.request.Request(site, headers=hdr)\n",
    "    html_page = urllib.request.urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "    links_in_page = [link.get('href') for link in soup.findAll('a')]\n",
    "    fff_latest_company_screens_url = [l for l in links_in_page if 'Invest+Your+Values+company+screens' in l][0]\n",
    "    print(\"\\n** Fetching latest Fossil Free Funds company screens list **\")\n",
    "    print(\"Using \"+fff_latest_company_screens_url)\n",
    "    return pd.read_excel(fff_latest_company_screens_url, sheet_name=1)\n",
    "\n",
    "\n",
    "def prepare_fff(df, fossil_only=False):\n",
    "    # Input: Fossil Free Funds list as dataframe\n",
    "    # Output:\n",
    "    # map fossil flags to 1/0 instead of Y/None\n",
    "    fff_cols = [c for c in df.columns if 'Fossil Free' in c]\n",
    "    df[fff_cols] = df[fff_cols].applymap(lambda x: 1 if x == 'Y' else 0)\n",
    "    # define fossil criteria := any of these are true: coal, oil / gas or fossil-fired utility\n",
    "    criteria = (df['Fossil Free Funds: Coal screen'] +\n",
    "                df['Fossil Free Funds: Oil / gas screen'] +\n",
    "                df['Fossil Free Funds: Fossil-fired utility screen']\n",
    "                ) > 0\n",
    "    df['fff_fossil_any'] = criteria.astype(int)\n",
    "    print(\"\\nis_fossil in Fossil Free Funds list\")\n",
    "    print(df['fff_fossil_any'].value_counts(dropna=False))\n",
    "    print(\"\\nFossil tags breakdown\")\n",
    "    print(\n",
    "        pd.crosstab(\n",
    "            df['Fossil Free Funds: Coal screen'],\n",
    "            [\n",
    "                df['Fossil Free Funds: Oil / gas screen'],\n",
    "                df['Fossil Free Funds: Fossil-fired utility screen']\n",
    "            ],\n",
    "            rownames=[\"Coal\"],\n",
    "            colnames=[\"Oil / Gas\", \"Utilities\"],\n",
    "            dropna=False\n",
    "        )\n",
    "    )\n",
    "    df['Company'] = df['Company'].str.upper().str.strip()\n",
    "    df['Tickers'] = df['Tickers'].str.upper().str.strip()\n",
    "    # narrow down to companies tagged as fossil only\n",
    "    if fossil_only:\n",
    "        fff = df[criteria]\n",
    "    else:\n",
    "        fff = df\n",
    "    # explode lists, to get one row per ticker\n",
    "    fff = fff.assign(Tickers=fff['Tickers'].str.split(',')).explode('Tickers')\n",
    "    # remove irrelevant columns\n",
    "    id_cols = [\"Company\", \"Country\", \"Tickers\"]\n",
    "    fff = fff[id_cols + fff_cols + ['fff_fossil_any']]\n",
    "    fff = fff[fff['Tickers'].notnull()]\n",
    "    fff['Tickers'] = fff['Tickers'].str.strip().str.upper()\n",
    "    return fff\n",
    "\n",
    "\n",
    "# TLV (TASE) companies list, maintained by Clean Money Forum\n",
    "# TODO: download file from a repository or db instead of using local\n",
    "def fetch_latest_tlv_list(tlv_path):\n",
    "    tlv = pd.read_excel(tlv_path, sheet_name=0, skiprows=range(3))\n",
    "    print(\"\\n** Fetching tlv companies fossil classification **\")\n",
    "    return tlv\n",
    "\n",
    "\n",
    "def prepare_tlv(tlv):\n",
    "    tlv.columns = tlv.columns.str.strip()\n",
    "    tlv[\"מספר מנפיק\"] = id_col_clean(tlv[\"מספר מנפיק\"])\n",
    "    tlv[\"מספר תאגיד\"] = id_col_clean(tlv[\"מספר תאגיד\"])\n",
    "\n",
    "    def ken_lo_to_binary(s):\n",
    "        if s.startswith('כן'):\n",
    "            return 1\n",
    "        elif s.startswith('לא'):\n",
    "            return 0\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    tlv[\"רשימה שחורה\"] = tlv[\"רשימה שחורה\"].map(ken_lo_to_binary)\n",
    "    print(\"\\nis_fossil in TLV companies classification\")\n",
    "    print(tlv[\"רשימה שחורה\"].value_counts(dropna=False))\n",
    "    print(\"\\n*** TLV companies with missing fossil classification ***\")\n",
    "    print(tlv[tlv[\"רשימה שחורה\"].isnull()])\n",
    "    return tlv\n",
    "\n",
    "\n",
    "# TODO: download file from a repository or db instead of using local\n",
    "def fetch_latest_prev_classified(prev_class_path):\n",
    "    prev_fossil_classification = pd.read_csv(prev_class_path)\n",
    "    return prev_fossil_classification\n",
    "\n",
    "\n",
    "# 3. Previously classified, adding issuers and LEI\n",
    "def prepare_prev_class(prev_class):\n",
    "    prev_class[\"security_num\"] = id_col_clean(prev_class[\"security_num\"])\n",
    "    prev_class[\"issuer_or_corp_num\"] = id_col_clean(prev_class[\"issuer_or_corp_num\"])\n",
    "    print(\"\\n** Previously classified ISINs and corps **\")\n",
    "    print(\"is_fossil in previously classified\")\n",
    "    print(prev_class[\"is_fossil\"].value_counts(dropna=False))\n",
    "    prev_grouped_by_sec_num = prev_class.groupby(\"security_num\")\n",
    "    sec_num_with_diff_class = prev_grouped_by_sec_num.filter(lambda x:\n",
    "                                                             (0 < x[\"is_fossil\"].mean() < 1)\n",
    "                                                             )\n",
    "# TODO: raise warning for ambiguously classified\n",
    "    if len(sec_num_with_diff_class) > 0:\n",
    "        print(\"\\n*** Securities with both is_fossil=1 and is_fossil=0 ***\")\n",
    "        print(sec_num_with_diff_class)\n",
    "\n",
    "    prev_grouped_by_issuer = prev_class[prev_class[\"issuer_or_corp_num\"] != 'NAN'].groupby(\"issuer_or_corp_num\")\n",
    "    issuer_with_diff_class = prev_grouped_by_issuer.filter(lambda x:\n",
    "                                                           (0 < x[\"is_fossil\"].mean() < 1)\n",
    "                                                           )\n",
    "    if len(issuer_with_diff_class) > 0:\n",
    "        print(\"\\n*** Issuers with both is_fossil=1 and is_fossil=0 ***\")\n",
    "        print(issuer_with_diff_class)\n",
    "    return prev_class\n",
    "\n",
    "\n",
    "def prepare_holdings(holdings_path, sheet_num):\n",
    "    if holdings_path.lower().endswith(\".xls\") | holdings_path.lower().endswith(\".xlsx\"):\n",
    "        # TODO: handle multiple sheets - run one by one?\n",
    "        holdings = pd.read_excel(holdings_path, sheet_name=sheet_num)\n",
    "    elif holdings_path.lower().endswith(\".csv\"):\n",
    "        holdings = pd.read_csv(holdings_path)\n",
    "    else:\n",
    "        # TODO: return error\n",
    "        print(\"holdings input file isn't Excel or CSV file\")\n",
    "        return\n",
    "    print(\"\\n** Holdings file for classification **\")\n",
    "    print(holdings_path)\n",
    "    holdings.columns = holdings.columns.str.strip()\n",
    "    print(\"columns: {}\".format(holdings.columns))\n",
    "    isin_col = find_isin_col(holdings)\n",
    "    holdings[isin_col] = id_col_clean(holdings[isin_col])\n",
    "    il_corp_col = find_il_corp_num_col(holdings)\n",
    "    if il_corp_col:\n",
    "        holdings[il_corp_col] = id_col_clean(holdings[il_corp_col])\n",
    "    return holdings, isin_col, il_corp_col\n",
    "\n",
    "\n",
    "def fetch_latest_tlv_sec_num_to_issuer():\n",
    "    # TODO: scrape from webpage\n",
    "    #  \"https://info.tase.co.il/_layouts/Tase/ManagementPages/Export.aspx?sn=none&GridId=106&AddCol=1&Lang=he-IL&CurGuid={6B3A2B75-39E1-4980-BE3E-43893A21DB05}&ExportType=3\"\n",
    "    df = pd.read_csv(\"/Users/urimarom/Downloads/Data_20210529.csv\",\n",
    "                     encoding=\"ISO-8859-8\",\n",
    "                     skiprows=3,\n",
    "                     dtype={\"מספר תאגיד\": str}\n",
    "                     )\n",
    "    # print(\"TLV sec num to issuer columns: {}\".format(df.columns))\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_latest_isin2lei(isin2lei_path):\n",
    "    # TODO: Fetch automatically from website\n",
    "    # https://www.gleif.org/en/lei-data/lei-mapping/download-isin-to-lei-relationship-files\n",
    "    isin2lei = pd.read_csv(isin2lei_path, index_col=1)\n",
    "    return isin2lei\n",
    "\n",
    "\n",
    "def prepare_tlv_sec_num_to_issuer(tlv_s2i):\n",
    "    tlv_s2i.columns = tlv_s2i.columns.str.strip()\n",
    "    tlv_s2i[\"\"\"מס' ני\"ע\"\"\"]= id_col_clean(tlv_s2i[\"\"\"מס' ני\"ע\"\"\"])\n",
    "    tlv_s2i[\"ISIN\"] = id_col_clean(tlv_s2i[\"ISIN\"])\n",
    "    tlv_s2i[\"מספר מנפיק\"] = id_col_clean(tlv_s2i[\"מספר מנפיק\"])\n",
    "    tlv_s2i[\"מספר תאגיד\"] = id_col_clean(tlv_s2i[\"מספר תאגיד\"])\n",
    "    return tlv_s2i\n",
    "\n",
    "\n",
    "def choose_best_issuer_num(row):\n",
    "    if not row[\"מספר מנפיק\"]:\n",
    "        return row[\"מספר מנפיק_x\"]\n",
    "    elif not row[\"מספר מנפיק_x\"]:\n",
    "        return row[\"מספר מנפיק\"]\n",
    "    else:\n",
    "        if (len(str(row[\"מספר מנפיק\"])) < len(str(row[\"מספר מנפיק_x\"]))) & (len(str(row[\"מספר מנפיק\"])) > 0):\n",
    "            return row[\"מספר מנפיק\"]\n",
    "        else:\n",
    "            return row[\"מספר מנפיק_x\"]\n",
    "\n",
    "\n",
    "def add_tlv_issuer_by_col(df, mapping, holdings_join_col, mapping_join_col):\n",
    "    mapping = mapping.groupby(mapping_join_col).first()\n",
    "    df_with_issuer = pd.merge(left=df,\n",
    "                              right=mapping['מספר מנפיק'],\n",
    "                              left_on=holdings_join_col,\n",
    "                              right_index=True,\n",
    "                              how='left'\n",
    "                              )\n",
    "    # TODO: use both original issuer number (if exists) and new one from mapping\n",
    "    if \"מספר מנפיק_y\" in df_with_issuer.columns:\n",
    "        # choose the more accurate issuer number\n",
    "        df_with_issuer.rename({\"מספר מנפיק_y\":\"מספר מנפיק\"}, axis=1, inplace=True)\n",
    "        df_with_issuer[\"מספר מנפיק\"] = id_col_clean(df_with_issuer[\"מספר מנפיק\"])\n",
    "        df_with_issuer[\"מספר מנפיק\"] = df_with_issuer.apply(choose_best_issuer_num, axis='columns')\n",
    "        df_with_issuer = df_with_issuer.drop(['מספר מנפיק_x'], axis=1)\n",
    "    df_with_issuer[\"מספר מנפיק\"] = id_col_clean(df_with_issuer[\"מספר מנפיק\"])\n",
    "    print(\"Holdings with matching issuer number after joining by {}: {} out of total holdings {}\".format(\n",
    "        holdings_join_col,\n",
    "        df_with_issuer[\"מספר מנפיק\"].notnull().sum(),\n",
    "        df_with_issuer.shape[0]\n",
    "    ))\n",
    "    return df_with_issuer\n",
    "\n",
    "\n",
    "def add_tlv_issuer_by_ticker(\n",
    "        df,\n",
    "        mapping,\n",
    "        df_isin_col,\n",
    "        df_ticker_col,\n",
    "        df_issuer_col,\n",
    "        mapping_heb_ticker_col,\n",
    "        mapping_eng_ticker_col\n",
    "):\n",
    "    # try exact match by ticker symbol, after removing trailing numbers\n",
    "    # deal only with TLV securities without an issuer\n",
    "    # handle hebrew and english separately\n",
    "    df_tlv_mask = is_tlv(df, df_isin_col)\n",
    "    df_tlv = df[df_tlv_mask]\n",
    "    # remove trailing digits from tickers\n",
    "    df_tlv[\"clean_ticker\"] = df_tlv[df_ticker_col].map(lambda s: clean_ticker(s))\n",
    "    # handle hebrew tickers\n",
    "    df_tlv_heb_ticker = df_tlv[df_tlv[df_ticker_col].map(lambda s: any_heb_char(s))]\n",
    "    # focus on tickers with no issuer\n",
    "    df_tlv_heb_ticker_no_issuer = df_tlv_heb_ticker[df_tlv_heb_ticker[df_issuer_col].isnull()]\n",
    "    mapping_heb = mapping[[mapping_heb_ticker_col, 'מספר מנפיק']]\n",
    "    mapping_heb[mapping_heb_ticker_col] = mapping_heb[mapping_heb_ticker_col].map(lambda s: clean_ticker(s))\n",
    "    mapping_heb = mapping_heb.groupby(mapping_heb_ticker_col).first()\n",
    "    merge_by_heb_ticker = pd.merge(\n",
    "        df_tlv_heb_ticker_no_issuer[[df_isin_col, \"clean_ticker\"]],\n",
    "        mapping_heb,\n",
    "        left_on=\"clean_ticker\",\n",
    "        right_on=mapping_heb_ticker_col,\n",
    "        how='inner'\n",
    "    )\n",
    "    # do the same for English tickers\n",
    "    mapping_eng = mapping[[mapping_eng_ticker_col, 'מספר מנפיק']]\n",
    "    mapping_eng[mapping_eng_ticker_col] = mapping_eng[mapping_eng_ticker_col].map(lambda s: clean_ticker(s))\n",
    "    mapping_eng = mapping_eng.groupby(mapping_eng_ticker_col).first()\n",
    "    df_tlv_no_issuer = df_tlv[df_tlv[df_issuer_col].isnull()]\n",
    "    merge_by_eng_ticker = pd.merge(\n",
    "        df_tlv_no_issuer[[df_isin_col, \"clean_ticker\"]],\n",
    "        mapping_eng,\n",
    "        left_on=\"clean_ticker\",\n",
    "        right_on=mapping_eng_ticker_col,\n",
    "        how='inner'\n",
    "    )\n",
    "    # put the results together\n",
    "    isin2issuer_through_ticker = pd.concat([merge_by_heb_ticker, merge_by_eng_ticker])\n",
    "    isin2issuer_through_ticker.rename({\"מספר מנפיק\": \"issuer_by_ticker\"}, axis=1, inplace=True)\n",
    "    isin2issuer_through_ticker = isin2issuer_through_ticker.groupby(df_isin_col).first()\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        isin2issuer_through_ticker[\"issuer_by_ticker\"],\n",
    "        left_on=df_isin_col,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    # use issuer by ticker to fill na in issure column\n",
    "    print(\"number of holdings with issuer before adding issuers by ticker: {}\".format(\n",
    "        df[df_issuer_col].notnull().sum()\n",
    "    ))\n",
    "    df[df_issuer_col] = df[df_issuer_col].fillna(df['issuer_by_ticker'])\n",
    "    print(\"number of holdings with issuer after adding issuers by ticker: {}\".format(\n",
    "        df[df_issuer_col].notnull().sum()\n",
    "    ))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_LEI_by_isin(df, mapping, df_isin_col):\n",
    "    df_with_lei = pd.merge(\n",
    "        left=df,\n",
    "        right=mapping,\n",
    "        left_on=df_isin_col,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    print(\"ISINs with matching LEI: {} out of total rows: {}\".format(\n",
    "        df_with_lei[\"LEI\"].notnull().sum(),\n",
    "        df_with_lei.shape[0]\n",
    "    ))\n",
    "    return df_with_lei\n",
    "\n",
    "\n",
    "# Matching functions: holdings with prev, TLV list, FFF list\n",
    "def match_holdings_with_prev(holdings, prev, holdings_isin_col, holdings_corp_or_issuer_col):\n",
    "    # 1. matching by ISIN / security number\n",
    "    print(\"\\n1. matching to previously classified by ISIN / security number\")\n",
    "    prev_sec_num = prev.groupby(\"security_num\").first()\n",
    "    holdings_prev_by_isin = pd.merge(left=holdings,\n",
    "                                     right=prev_sec_num['is_fossil'],\n",
    "                                     left_on=holdings_isin_col,\n",
    "                                     right_index=True,\n",
    "                                     how='left'\n",
    "                                     )\n",
    "    holdings_prev_by_isin.rename({\"is_fossil\": \"is_fossil_prev_ISIN\"}, axis=1, inplace=True)\n",
    "    print(\"\\nprevious is_fossil coverage\")\n",
    "    print(\"ISINs previously classified: {} out of total holdings: {}\".format(\n",
    "        holdings_prev_by_isin[\"is_fossil_prev_ISIN\"].notnull().sum(),\n",
    "        holdings_prev_by_isin.shape[0]\n",
    "    ))\n",
    "    # 2. by issuer number\n",
    "    print(\"\\n2. matching to previously classified by issuer number\")\n",
    "    prev_issuer = prev.groupby(\"מספר מנפיק\").first()\n",
    "    holdings_prev_by_issuer = pd.merge(left=holdings_prev_by_isin,\n",
    "                                       right=prev_issuer['is_fossil'],\n",
    "                                       left_on=\"מספר מנפיק\",\n",
    "                                       right_index=True,\n",
    "                                       how='left'\n",
    "                                       )\n",
    "    holdings_prev_by_issuer.rename({\"is_fossil\": \"is_fossil_prev_issuer\"}, axis=1, inplace=True)\n",
    "    print(\"issuers previously classified: {} out of total holdings: {}\".format(\n",
    "        holdings_prev_by_issuer[\"is_fossil_prev_issuer\"].notnull().sum(),\n",
    "        holdings_prev_by_issuer.shape[0]\n",
    "    ))\n",
    "    # 3. by LEI - (Legal Entity Identifier, international)\n",
    "    print(\"\\n3. matching to previously classified by LEI\")\n",
    "    prev_LEI = prev.groupby(\"LEI\").first()\n",
    "    holdings_prev = pd.merge(left=holdings_prev_by_issuer,\n",
    "                             right=prev_LEI['is_fossil'],\n",
    "                             left_on=\"LEI\",\n",
    "                             right_index=True,\n",
    "                             how='left'\n",
    "                             )\n",
    "    holdings_prev.rename({\"is_fossil\": \"is_fossil_prev_LEI\"}, axis=1, inplace=True)\n",
    "    print(\"LEIs previously classified: {} out of total holdings: {}\".format(\n",
    "        holdings_prev[\"is_fossil_prev_LEI\"].notnull().sum(),\n",
    "        holdings_prev.shape[0]\n",
    "    ))\n",
    "    return holdings_prev\n",
    "\n",
    "\n",
    "def match_holdings_with_tlv(holdings, tlv, holdings_corp_or_issuer_col):\n",
    "    # join on issuer number\n",
    "    holdings_with_tlv = pd.merge(left=holdings,\n",
    "                                 right=tlv[['מספר מנפיק', 'רשימה שחורה']],\n",
    "                                 right_on='מספר מנפיק',\n",
    "                                 left_on=\"מספר מנפיק\",\n",
    "                                 how='left'\n",
    "                                 )\n",
    "    holdings_with_tlv.rename({\"רשימה שחורה\": \"is_fossil_il_list_issuer\"}, axis=1, inplace=True)\n",
    "    holdings_with_tlv[\"is_fossil_il_list_issuer\"].value_counts(dropna=False)\n",
    "    print(\"\\nTLV list is_fossil coverage\")\n",
    "    print(\"classified: {} out of total holdings: {}\".format(\n",
    "        holdings_with_tlv[\"is_fossil_il_list_issuer\"].notnull().sum(),\n",
    "        holdings_with_tlv.shape[0]\n",
    "    ))\n",
    "    return holdings_with_tlv\n",
    "\n",
    "\n",
    "def match_holdings_with_fff_by_ticker(\n",
    "        holdings,\n",
    "        fff,\n",
    "        holdings_ticker_col,\n",
    "        holdings_company_col,\n",
    "        fff_company_col=\"Company\",\n",
    "        match_threshold=80):\n",
    "    holdings_without_ticker = holdings[holdings[holdings_ticker_col].isnull()]\n",
    "    print(\"Holdings without ticker: {}\".format(holdings_without_ticker.shape[0]))\n",
    "    holdings_with_ticker = holdings[holdings[holdings_ticker_col].notnull()]\n",
    "    print(\"Holdings with ticker: {}\".format(holdings_with_ticker.shape[0]))\n",
    "    holdings_with_ticker[\"clean_ticker\"] = holdings_with_ticker[holdings_ticker_col].map(lambda s: clean_ticker(s))\n",
    "    fff[\"clean_ticker\"] = fff[\"Tickers\"].map(lambda s: clean_ticker(s))\n",
    "    fff = fff[fff[\"clean_ticker\"].notnull()]\n",
    "    fff_one_per_ticker = fff.groupby([\"clean_ticker\", fff_company_col]).first().reset_index()\n",
    "    fff_one_per_ticker[\"clean_ticker\"] = id_col_clean(fff_one_per_ticker[\"clean_ticker\"])\n",
    "    fff_one_per_ticker = fff_one_per_ticker[fff_one_per_ticker[\"clean_ticker\"].notnull()]\n",
    "    fff_one_per_ticker = fff_one_per_ticker.set_index('clean_ticker')\n",
    "    holdings_with_ticker[\"clean_ticker\"] = id_col_clean(holdings_with_ticker[\"clean_ticker\"])\n",
    "    holdings_with_fff_by_ticker = pd.merge(\n",
    "        holdings_with_ticker,\n",
    "        fff_one_per_ticker[[fff_company_col, 'fff_fossil_any']],\n",
    "        left_on=\"clean_ticker\",\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    holdings_with_fff_by_ticker.rename({fff_company_col: 'fff_company_by_ticker'}, axis=1, inplace=True)\n",
    "    # adding fuzzy matching between holdings company name and fff company name to discard false positives by ticker\n",
    "    holdings_with_fff_by_ticker['ticker_company_match_score'] = holdings_with_fff_by_ticker.apply(\n",
    "        lambda row: company_names_match_score(\n",
    "            row,\n",
    "            holdings_company_col=holdings_company_col,\n",
    "            fff_company_col='fff_company_by_ticker'\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "    # take ticker matches with maximal company name match\n",
    "    got_ticker_matches = holdings_with_fff_by_ticker[holdings_with_fff_by_ticker['fff_company_by_ticker'].notnull()]\n",
    "    no_ticker_matches = holdings_with_fff_by_ticker[holdings_with_fff_by_ticker['fff_company_by_ticker'].isnull()]\n",
    "    # done by desc sorting by company score and then de-duping to keep the rows with max score per holding\n",
    "    got_ticker_matches = got_ticker_matches.sort_values(\n",
    "        'ticker_company_match_score', ascending=False).drop_duplicates(\n",
    "        got_ticker_matches.columns.drop(['fff_company_by_ticker', 'fff_fossil_any', 'ticker_company_match_score'])\n",
    "    )\n",
    "    holdings_with_fff_by_ticker = pd.concat([got_ticker_matches, no_ticker_matches])\n",
    "    holdings_with_fff_by_ticker[\"is_fossil_fff_ticker\"] = holdings_with_fff_by_ticker.apply(\n",
    "        lambda row: row['fff_fossil_any'] if row['ticker_company_match_score'] > match_threshold else np.nan,\n",
    "        axis='columns'\n",
    "    )\n",
    "    # rename columns\n",
    "    holdings_with_fff_by_ticker = holdings_with_fff_by_ticker.rename({\"fff_fossil_any\": \"fff_by_ticker_fossil\"}, axis=1)\n",
    "    holdings_with_fff_by_ticker = pd.concat([holdings_with_fff_by_ticker, holdings_without_ticker])\n",
    "    print(\"Matching by Ticker coverage:\")\n",
    "    print(\"classified: {} out of total holdings: {}\".format(\n",
    "        holdings_with_fff_by_ticker[\"is_fossil_fff_ticker\"].notnull().sum(),\n",
    "        holdings_with_fff_by_ticker.shape[0]\n",
    "    ))\n",
    "    return holdings_with_fff_by_ticker\n",
    "\n",
    "\n",
    "def best_match(s, l, first_word_thresh=95):\n",
    "    s = str(s)\n",
    "    # if there's a perfect match, it's the winner\n",
    "    if s in l:\n",
    "        return s, 100\n",
    "    # start with matching the first word (most indicative)\n",
    "    if len(s) > 0:\n",
    "        first_word_matches = process.extract(s.split()[0], l, scorer=fuzz.partial_ratio, limit=10)\n",
    "    else:\n",
    "        return '', 0\n",
    "    # go over candidates with good first word match, get fuzzy match score for each and choose winner\n",
    "    max_agg_score = 0\n",
    "    winner = ''\n",
    "    for m in first_word_matches:\n",
    "        if m[1] > first_word_thresh:\n",
    "            agg_score = (\n",
    "                fuzz.ratio(s, m[0]) +\n",
    "                fuzz.partial_ratio(s, m[0]) +\n",
    "                fuzz.token_sort_ratio(s, m[0]) +\n",
    "                fuzz.token_set_ratio(s, m[0]) +\n",
    "                fuzz.partial_token_sort_ratio(s, m[0]) +\n",
    "                fuzz.partial_token_set_ratio(s, m[0])\n",
    "            )\n",
    "            if agg_score > max_agg_score:\n",
    "                max_agg_score = agg_score\n",
    "                winner = m[0]\n",
    "    # normalize score to be 0-100\n",
    "    final_score = max_agg_score / 6\n",
    "    return winner, final_score\n",
    "\n",
    "\n",
    "def match_holdings_with_fff_by_company_name(\n",
    "        holdings,\n",
    "        fff,\n",
    "        common_words_in_company,\n",
    "        holdings_company_col,\n",
    "        fff_company_col=\"Company\",\n",
    "        min_match_threshold=60,\n",
    "        is_fossil_match_threshold=90\n",
    "        ):\n",
    "    # prepare company names for fuzzy matching\n",
    "    # remove common words (LTD, Corp etc.)\n",
    "    holdings[\"company_clean\"] = holdings[holdings_company_col].map(lambda s: clean_company(s))\n",
    "    holdings[\"company_clean\"] = remove_common_words(holdings[\"company_clean\"], common_words_in_company)\n",
    "    # TODO: maybe use ASA, PLC, INC etc. as separator? remove everything after separator if got >= n (3?) words\n",
    "    holdings_company_names = holdings[\"company_clean\"].dropna().str.upper().str.strip().unique()\n",
    "    fff[\"company_clean\"] = remove_common_words(fff[fff_company_col], common_words_in_company)\n",
    "    fff[\"company_clean\"] = fff[\"company_clean\"].str.upper().str.strip()\n",
    "    fff_company_names = fff[\"company_clean\"].dropna().unique()\n",
    "    # fuzzy matching company names\n",
    "    print(\"\\n** fuzzy matching company names ** (this could take a few minutes)\")\n",
    "    agg_matches = {}\n",
    "    for c in holdings_company_names:\n",
    "        agg_matches[c] = best_match(c, fff_company_names)\n",
    "    agg_fuzzy_results = pd.DataFrame(agg_matches).transpose()\n",
    "    agg_fuzzy_results.rename({0: 'fff_by_name', 1: 'company_name_match_score'}, axis=1, inplace=True)\n",
    "    agg_fuzzy_results = agg_fuzzy_results[agg_fuzzy_results['company_name_match_score'] > min_match_threshold]\n",
    "    # join back to fff to get fff_fossil_any\n",
    "    fff_company_with_fff_fossil_any = fff.groupby('company_clean').first()\n",
    "    agg_fuzzy_results = pd.merge(\n",
    "        left=agg_fuzzy_results,\n",
    "        right=fff_company_with_fff_fossil_any['fff_fossil_any'],\n",
    "        left_on='fff_by_name',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    # add fuzzy match results to holdings\n",
    "    holdings_with_fuzzy = pd.merge(\n",
    "        left=holdings,\n",
    "        right=agg_fuzzy_results,\n",
    "        left_on=\"company_clean\",\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    holdings_with_fuzzy[\"is_fossil_company_name\"] = holdings_with_fuzzy.apply(\n",
    "        lambda row: row['fff_fossil_any'] if row['company_name_match_score'] > is_fossil_match_threshold else np.nan,\n",
    "        axis='columns'\n",
    "    )\n",
    "    # rename columns\n",
    "    holdings_with_fuzzy = holdings_with_fuzzy.rename({'fff_fossil_any': 'fff_by_name_fossil'}, axis=1)\n",
    "    # drop redundant columns\n",
    "    if 'company_name_cut_ticker' in holdings_with_fuzzy.columns:\n",
    "        holdings_with_fuzzy = holdings_with_fuzzy.drop(['company_name_cut_ticker'], axis=1)\n",
    "    print(\"Matching by Company Name coverage:\")\n",
    "    print(\"classified: {} out of total holdings: {}\".format(\n",
    "        holdings_with_fuzzy[\"is_fossil_company_name\"].notnull().sum(),\n",
    "        holdings_with_fuzzy.shape[0]\n",
    "    ))\n",
    "    return holdings_with_fuzzy\n",
    "\n",
    "\n",
    "# is_fossil consolidation - using multiple is_fossil_x flags to get is_fossil\n",
    "def consolidate_is_fossil(df):\n",
    "    # produces final is_fossil flag, based on all the sub flags\n",
    "    # adds is_fossil_reason and is_fossil_certainty (=1 for all except FFF fuzzy matching)\n",
    "    is_fossil_cols = [c for c in df.columns if c.startswith(\"is_fossil\")]\n",
    "    # if any fossil flag is true, is_fossil := True\n",
    "    # if all available fossil flags are false, is_fossil := False\n",
    "    # if there are no available fossil flags, is_fossil := null\n",
    "    # TODO: handle rows with mixed flags (0 and 1)\n",
    "    df[\"is_fossil\"] = df[is_fossil_cols].max(axis=1)\n",
    "    print(\"\\n***** Final Results before propagation *****\")\n",
    "    print(\"is_fossil coverage:\")\n",
    "    print(df[\"is_fossil\"].value_counts(dropna=False))\n",
    "    return df\n",
    "\n",
    "\n",
    "# propagate is_fossil across same identity (ISIN, LEI)\n",
    "def propagate_is_fossil(df, propagate_by_col):\n",
    "    # use freshly classified holdings to classify others with similar ISINs or LEIs\n",
    "    print(\"\\nPropagating by {}\".format(propagate_by_col))\n",
    "    df = df.reset_index(drop=True)\n",
    "    prop_col_null = df[df[propagate_by_col].isnull()]\n",
    "    prop_col_not_null = df[df[propagate_by_col].notnull()]\n",
    "    grouped_by_prop_col = prop_col_not_null.sort_values(propagate_by_col).groupby(propagate_by_col)\n",
    "    # HAVING different is_fossil values, including nulls\n",
    "    # fossil_partially_missing = grouped_by_prop_col.filter(lambda x: x[\"is_fossil\"].nunique(dropna=False) > 1)\n",
    "    # if len(fossil_partially_missing) >0:\n",
    "    #     print(\"\\nHAVING different is_fossil values within group, including nulls (partially missing classification)\")\n",
    "    #     print(grouped_by_prop_col.filter(lambda x: x[\"is_fossil\"].nunique(dropna=False) > 1))\n",
    "    # HAVING both is_fossil=0 and is_fossil=1 values\n",
    "    fossil_ambiguous = grouped_by_prop_col.filter(lambda x: 0 < x[\"is_fossil\"].mean() < 1)\n",
    "    # TODO: Warning - multiple is_fossil values for the same entity\n",
    "    if len(fossil_ambiguous) > 0:\n",
    "        print(\"\\nHAVING both is_fossil=0 and is_fossil=1 values within group\")\n",
    "        print(fossil_ambiguous)\n",
    "    # propagate mean to missing is_fossil\n",
    "    prop_col_not_null['is_fossil'] = grouped_by_prop_col['is_fossil'].transform(lambda x: x.fillna(x.mean()))\n",
    "    result = pd.concat([prop_col_not_null, prop_col_null])\n",
    "    print(\"\\nis_fossil coverage before propagation by {}:\".format(propagate_by_col))\n",
    "    print(df[\"is_fossil\"].value_counts(dropna=False))\n",
    "    print(\"\\nis_fossil coverage after propagation by {}:\".format(propagate_by_col))\n",
    "    print(result[\"is_fossil\"].value_counts(dropna=False))\n",
    "    return result\n",
    "\n",
    "\n",
    "# TODO: upload csv to Google Drive or other repository\n",
    "def output(df, output_path):\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\nWriting results to {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Holdings file for classification **\n",
      "/Users/urimarom/Downloads/חשיפה לפוסיליים - Q42020 - החזקות ישירות באגח קונצרני.csv\n",
      "columns: Index(['שם המנפיק/שם נייר ערך', 'מספר ני\"ע', 'זירת מסחר', 'ספק מידע',\n",
      "       'מספר מנפיק_z', 'ענף מסחר', 'דירוג', 'שם מדרג', 'תאריך רכישה', 'מח\"מ',\n",
      "       'סוג מטבע', 'שיעור ריבית', 'תשואה לפידיון', 'ערך נקוב****', 'שער***',\n",
      "       'פדיון/ ריבית/ דיבידנד לקבל*****', 'שווי שוק', 'שעור מערך נקוב מונפק',\n",
      "       'שעור מנכסי אפיק ההשקעה', 'שעור מסך נכסי השקעה**', 'גוף', 'אפיק',\n",
      "       'מספר מנפיק', 'LEI', 'is_fossil_prev_ISIN', 'is_fossil_prev_issuer',\n",
      "       'is_fossil_prev_LEI', 'is_fossil_il_list_issuer_orig', 'company_clean',\n",
      "       'fff_by_name', 'company_name_match_score', 'fff_by_name_fossil',\n",
      "       'is_fossil_company_name', 'is_fossil', 'שווי פוסילי', 'Unnamed: 35'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urimarom/anaconda3/lib/python3.6/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holding file ISIN col is: מספר ני\"ע\n",
      "number of ISINs: 1741 out of 8323 rows\n",
      "\n",
      "Holding file Israel Corp col is: מספר מנפיק_z\n",
      "number of Israel Corp Numbers: 6021 out of 8323 rows\n",
      "Holdings with matching issuer number after joining by מספר ני\"ע: 5953 out of total holdings 8323\n",
      "\n",
      "** Fetching tlv companies fossil classification **\n",
      "\n",
      "is_fossil in TLV companies classification\n",
      "0    541\n",
      "1     50\n",
      "Name: רשימה שחורה, dtype: int64\n",
      "\n",
      "*** TLV companies with missing fossil classification ***\n",
      "Empty DataFrame\n",
      "Columns: [מספר מנפיק, מספר תאגיד, שם, שם מלא, ענף בבורסה, מדד תל אביב 125, רשימה שחורה, קריטריון פסילה, מדוע נפסל + מקור, כתובת, טלפון, אתר הבית, דואר אלקטרוני]\n",
      "Index: []\n",
      "\n",
      "TLV list is_fossil coverage\n",
      "classified: 5939 out of total holdings: 8323\n"
     ]
    }
   ],
   "source": [
    "holdings_path = \"/Users/urimarom/Downloads/חשיפה לפוסיליים - Q42020 - החזקות ישירות באגח קונצרני.csv\"\n",
    "tlv_path = \"/Users/urimarom/Downloads/כל החברות 2021 - 15.6.21.xlsx\"\n",
    "holdings_corp_or_issuer_col = \"מספר מנפיק\"\n",
    "holdings_ticker_col = None\n",
    "holdings_company_col = \"שם המנפיק/שם נייר ערך\"\n",
    "sheet_num=0\n",
    "\n",
    "# 1. prepare holdings file for classification\n",
    "holdings, holdings_isin_col, holdings_il_corp_col = prepare_holdings(holdings_path, sheet_num=sheet_num)\n",
    "# If ticker exists, remove ticker information from instrument name\n",
    "if holdings_ticker_col:\n",
    "    holdings = clean_instrument_from_ticker(holdings, holdings_company_col, holdings_ticker_col)\n",
    "    holdings_company_col = \"company_name_cut_ticker\"\n",
    "# 2. prepare mapping files: TLV security number to issuer & isin to LEI for international holdings\n",
    "tlv_s2i = prepare_tlv_sec_num_to_issuer(fetch_latest_tlv_sec_num_to_issuer())\n",
    "\n",
    "# 3. add issuer and LEI to holdings file\n",
    "holdings_with_issuer = add_tlv_issuer_by_col(holdings, tlv_s2i, holdings_join_col=holdings_isin_col,mapping_join_col=\"\"\"מס' ני\"ע\"\"\")\n",
    "# holdings_with_issuer = add_tlv_issuer_by_col(holdings_with_issuer, tlv_s2i, holdings_join_col=holdings_il_corp_col, mapping_join_col=\"מספר תאגיד\")\n",
    "if holdings_ticker_col:\n",
    "    holdings_with_issuer = add_tlv_issuer_by_ticker(\n",
    "        holdings_with_issuer,\n",
    "        tlv_s2i,\n",
    "        df_isin_col=holdings_isin_col,\n",
    "        df_issuer_col=\"מספר מנפיק\",\n",
    "        df_ticker_col=holdings_ticker_col,\n",
    "        mapping_heb_ticker_col=\"סימול(עברית)\",\n",
    "        mapping_eng_ticker_col=\"סימול(אנגלית)\"\n",
    "    )\n",
    "\n",
    "# 4. match with new TLV list\n",
    "tlv = prepare_tlv(fetch_latest_tlv_list(tlv_path))\n",
    "holdings_with_tlv = match_holdings_with_tlv(holdings_with_issuer, tlv, holdings_corp_or_issuer_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    5232\n",
       "1.0     707\n",
       "Name: is_fossil_il_list_issuer, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdings_with_tlv[\"is_fossil_il_list_issuer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    5294\n",
       "1.0     556\n",
       "Name: is_fossil_il_list_issuer_orig, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdings_with_tlv[\"is_fossil_il_list_issuer_orig\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to /Users/urimarom/Downloads/חשיפה לפוסיליים - Q42020 - החזקות ישירות באגח קונצרני with new IL fossil classification.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = ''.join(holdings_path.split('.')[:-1]) + ' with new IL fossil classification.' + holdings_path.split('.')[-1]\n",
    "output(holdings_with_tlv, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
