{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Fossil Classification for a given Holding File\n",
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fossil_classification import *\n",
    "from enrich_holdings import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify quarterly holdings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add better matching for non-Israeli companies, using Figi as name normalization, fall back to fuzzy matching on normalized name and then fuzzy matching on original name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "company_or_fund_level = \"company\"\n",
    "year = \"2023\"\n",
    "q = \"1\"\n",
    "folder_path = \"data/downloaded reports/\" + company_or_fund_level + \" reports/\" + year + \"Q\" + q + \"/\"\n",
    "holdings_path = folder_path + \"holdings_for_cls.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Preparing holding file\n",
      "\n",
      "** Holdings file for classification **\n",
      "data/downloaded reports/company reports/2023Q1/holdings_for_cls.csv\n",
      "columns: Index(['שם המנפיק/שם נייר ערך', 'מספר ני\"ע', 'מספר מנפיק', 'שווי',\n",
      "       'שעור מנכסי אפיק ההשקעה', 'שעור מסך נכסי השקעה', 'holding_type',\n",
      "       'זירת מסחר', 'תאריך רכישה', 'ערך נקוב', 'שער', 'שעור מערך נקוב מונפק',\n",
      "       'ענף מסחר', 'SystemName', 'ParentCorpName', 'ReportPeriodDesc'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urimarom/PycharmProjects/fossil_classification/fossil_classification.py:79: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  isin_cnt = sum(df[col].astype(str).str.strip().str.contains(isin_pattern, na=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holding file ISIN col is: מספר ני\"ע\n",
      "number of ISINs: 7452 out of 27665 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urimarom/PycharmProjects/fossil_classification/fossil_classification.py:101: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  pattern_cnt = sum(df[col].astype(str).str.strip().str.contains(pattern, na=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holding file Israel Corp col is: מספר מנפיק\n",
      "number of Israel Corp Numbers: 19146 out of 27665 rows\n",
      "\n",
      "2. Preparing mapping files\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclassify_holdings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mholdings_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/fossil_classification/fossil_classification.py:756\u001b[0m, in \u001b[0;36mclassify_holdings\u001b[0;34m(holdings_path, holdings_ticker_col, holdings_company_col, sheet_num, skip_fff)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. Preparing mapping files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m tlv_s2i \u001b[38;5;241m=\u001b[39m prepare_tlv_sec_num_to_issuer(fetch_latest_tlv_sec_num_to_issuer())\n\u001b[0;32m--> 756\u001b[0m isin2lei \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_latest_isin2lei\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# 3. enrich holdings file\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. Enriching holding file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/fossil_classification/enrich_holdings.py:250\u001b[0m, in \u001b[0;36mfetch_latest_isin2lei\u001b[0;34m(isin2lei_path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_latest_isin2lei\u001b[39m(isin2lei_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_sources/ISIN_LEI.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# TODO: Fetch automatically from website\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# https://www.gleif.org/en/lei-data/lei-mapping/download-isin-to-lei-relationship-files\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     isin2lei \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43misin2lei_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m isin2lei\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "classify_holdings(holdings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Review\n",
    "In a google spreadsheet or excel.\n",
    "Download the fully classifed file into a csv, then use it in holding_cls_path to update prev_class (see below).\n",
    "\n",
    "## Tips\n",
    "1. Look at the output of the script, review conflicting classification (by ISIN, LEI, Israeli security number)\n",
    "2. Look at holdings that get is_fossil_conflict=True\n",
    "3. Sort by security name, Israeli security number or ISIN for faster manual classification\n",
    "4. Carefully review holdings that have only is_fossil by FFF name match, as there are false matches.\n",
    "<br>Review both holdings for which all the other is_fossil_x flags are null, and such that have is_fossil by FFF = 1and other is_fossil_x = 0\n",
    "5. Review holdings from suspicious industries: energy, oil and gas, utilities, materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add classification results to prev_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding classifications to prev_class, saving the previous version as data_sources/prev_class backup/prev_class 2023-07-17 16-00-21.csv\n"
     ]
    }
   ],
   "source": [
    "holdings_cls_path = folder_path + \"holdings_for_cls with fossil classification - reviewed.csv\"\n",
    "prev_class_path = \"data_sources/prev_class.csv\"\n",
    "update_prev_class(holdings_cls_path, prev_class_path)\n",
    "# prev_class_fixed = add_all_id_types_to_holdings(prev_class, tlv_s2i, isin2lei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify fund holdings\n",
    "## Israeli funds\n",
    "Data is scraped from https://mayaapi.tase.co.il/api/fund/details?fundId=\n",
    "<br>Page address: https://maya.tase.co.il/fund/5132287?view=assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_directory = \"data/holdings_for_classification/5132287/\"\n",
    "response_path = response_directory + \"response.json\"\n",
    "fund = pd.read_json(response_path, orient=\"index\")\n",
    "assets = pd.DataFrame(fund.loc[\"AssetCompostion\"][0]['Assets'])\n",
    "# holdings[\"AssetCompostion\"].head()\n",
    "cols_rename = {\n",
    "    'AssetName': 'שם המנפיק/שם נייר ערך',\n",
    "    'IdentityCd': 'מספר ני\"ע',\n",
    "    'Id': 'fund_id'\n",
    "}\n",
    "assets = assets.rename(cols_rename, axis=1)\n",
    "assets[\"מספר מנפיק\"] = '00'\n",
    "assets[\"מספר תאגיד\"] = '00'\n",
    "assets.to_csv(response_directory+\"assets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_holdings(response_directory+\"assets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## holdings CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_csv_dir = \"/Users/urimarom/PycharmProjects/fossil_classification/data/holdings_for_classification/IE000PSF3A70/\"\n",
    "holdings_filename = 'fund_weights.csv'\n",
    "holdings_csv_path = holdings_csv_dir + holdings_filename\n",
    "holdings = pd.read_csv(holdings_csv_path)\n",
    "cols_rename = {\n",
    "    'Name': 'שם המנפיק/שם נייר ערך',\n",
    "    'ISIN': 'מספר ני\"ע',\n",
    "    'Type of Security': 'holding_type'\n",
    "}\n",
    "holdings = holdings.rename(cols_rename, axis=1)\n",
    "# fix missing columns\n",
    "holdings[\"מספר מנפיק\"] = '00'\n",
    "holdings[\"מספר תאגיד\"] = '00'\n",
    "holdings[\"is_fossil_prev_il_sec_num\"] = np.nan\n",
    "holdings.to_csv(holdings_csv_dir+\"fund_weights_fixed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_holdings(holdings_csv_dir+\"fund_weights_fixed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add additional screens - under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_path = \"data/downloaded reports/company reports/2022Q4/holdings_for_cls.csv\"\n",
    "holdings_ticker_col=None\n",
    "holdings_company_col=\"שם המנפיק/שם נייר ערך\"\n",
    "sheet_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. prepare holdings file for classification\n",
    "print(\"\\n1. Preparing holding file\")\n",
    "holdings, holdings_il_sec_num_col, holdings_il_corp_col = prepare_holdings(holdings_path, sheet_num=sheet_num)\n",
    "# If ticker exists, remove ticker information from instrument name\n",
    "if holdings_ticker_col:\n",
    "    holdings = clean_instrument_from_ticker(holdings, holdings_company_col, holdings_ticker_col)\n",
    "    holdings_company_col = \"company_name_cut_ticker\"\n",
    "# 2. prepare mapping files: TLV security number to issuer & isin to LEI for international holdings\n",
    "print(\"\\n2. Preparing mapping files\")\n",
    "tlv_s2i = prepare_tlv_sec_num_to_issuer(fetch_latest_tlv_sec_num_to_issuer())\n",
    "isin2lei = fetch_latest_isin2lei()\n",
    "# 3. enrich holdings file\n",
    "print(\"\\n3. Enriching holding file\")\n",
    "holdings_enriched = add_all_id_types_to_holdings(holdings, tlv_s2i, isin2lei)\n",
    "if holdings_ticker_col:\n",
    "    holdings_enriched = add_tlv_issuer_by_ticker(\n",
    "        holdings_enriched,\n",
    "        tlv_s2i,\n",
    "        df_isin_col=holdings_il_sec_num_col,\n",
    "        df_issuer_col=\"מספר מנפיק\",\n",
    "        df_ticker_col=holdings_ticker_col,\n",
    "        mapping_heb_ticker_col=\"סימול(עברית)\",\n",
    "        mapping_eng_ticker_col=\"סימול(אנגלית)\"\n",
    "    )\n",
    "# 4. prepare previously classified as is_fossil\n",
    "# print(\"\\n4. Preparing previously classified file\")\n",
    "# prev_class = prepare_prev_class(fetch_latest_prev_classified())\n",
    "# prev_class = add_all_id_types_to_holdings(prev_class, tlv_s2i, isin2lei)\n",
    "# 5. match holdings with previously classified - by ISIN, issuer or LEI\n",
    "# print(\"\\n5. Matching holdings with previously classified\")\n",
    "# holdings_with_prev = match_holdings_with_prev(\n",
    "#     holdings_enriched,\n",
    "#     prev_class,\n",
    "#     holdings_il_sec_num_col\n",
    "# )\n",
    "# tlv = prepare_tlv(fetch_latest_tlv_list())\n",
    "# holdings_with_tlv = match_holdings_with_tlv(holdings_with_prev, tlv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n6. Preparing Fossil Free Funds company list\")\n",
    "fff_all = fetch_latest_fff_list()\n",
    "fff_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = get_common_words_in_company_name(\n",
    "        holdings_enriched,\n",
    "        fff_all,\n",
    "        holdings_company_col=holdings_company_col,\n",
    "        fff_company_col=\"Company\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags_family = 'Deforestation Free Funds'\n",
    "# flags = [c for c in fff_all.columns if c.startswith(flags_family)]\n",
    "flags = [\n",
    "    'Deforestation Free Funds: Producer screen', # Leave in?\n",
    "    'Deforestation Free Funds: Financier screen', # Leave in?\n",
    "    'Deforestation Free Funds: Consumer brand screen', # Leave in?\n",
    "    'Gun Free Funds: Gun manufacturers screen',\n",
    "    'Gun Free Funds: Gun retailers screen', # Leave in?\n",
    "    'Weapons Free Funds: Major military contractor screen',\n",
    "    'Weapons Free Funds: Cluster munitions / landmines screen',\n",
    "    'Weapons Free Funds: Nuclear weapons screen',\n",
    "    'Tobacco Free Funds: Tobacco producers screen'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized version of the function - working on any set of flags\n",
    "def prepare_fff(df, flags, flagged_only=False):\n",
    "    # Input: Fossil Free Funds list as dataframe\n",
    "    # Output:\n",
    "    # map flags to 1/0 instead of Y/None\n",
    "    df[flags] = df[flags].applymap(lambda x: 1 if x == 'Y' else 0)\n",
    "    # define fossil criteria := any of these are true: coal, oil / gas or fossil-fired utility\n",
    "    criteria = df[flags].sum(axis=1) > 0\n",
    "    df['fff_flag_any'] = criteria.astype(int)\n",
    "    print(\"\\ncompanies with any of the chosen flags in Fossil Free Funds list\")\n",
    "    print(df['fff_flag_any'].value_counts(dropna=False))\n",
    "    print(\"\\nFlags breakdown\")\n",
    "    print(\n",
    "        pd.crosstab(\n",
    "            df[flags[0]],\n",
    "            [\n",
    "                df[flags[1]],\n",
    "                df[flags[2]]\n",
    "               ],\n",
    "            rownames=[flags[0]],\n",
    "            colnames=[flags[1], flags[2]],\n",
    "            dropna=False\n",
    "        )\n",
    "    )\n",
    "    df['Company'] = df['Company'].str.upper().str.strip()\n",
    "    df['Tickers'] = df['Tickers'].str.upper().str.strip()\n",
    "    # narrow down to companies tagged as fossil only\n",
    "    if flagged_only:\n",
    "        fff = df[criteria]\n",
    "    else:\n",
    "        fff = df\n",
    "    # explode lists, to get one row per ticker\n",
    "    fff = fff.assign(Tickers=fff['Tickers'].str.split(',')).explode('Tickers')\n",
    "    # remove irrelevant columns\n",
    "    id_cols = [\"Company\", \"Country\", \"Tickers\"]\n",
    "    fff = fff[id_cols + flags + ['fff_flag_any']]\n",
    "    fff = fff[fff['Tickers'].notnull()]\n",
    "    fff['Tickers'] = fff['Tickers'].str.strip().str.upper()\n",
    "    return fff\n",
    "\n",
    "fff = prepare_fff(fff_all, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_holdings_with_fff_by_company_name(\n",
    "        holdings,\n",
    "        fff,\n",
    "        common_words_in_company,\n",
    "        holdings_company_col,\n",
    "        flags,\n",
    "        agg_flag_name,\n",
    "        fff_company_col=\"Company\",\n",
    "        min_match_threshold=60,\n",
    "        flagged_match_threshold=90\n",
    "):\n",
    "    # prepare company names for fuzzy matching\n",
    "    # remove common words (LTD, Corp etc.)\n",
    "    holdings[\"company_clean\"] = holdings[holdings_company_col].map(lambda s: clean_company(s))\n",
    "    holdings[\"company_clean\"] = remove_common_words(holdings[\"company_clean\"], common_words_in_company)\n",
    "    # TODO: maybe use ASA, PLC, INC etc. as separator? remove everything after separator if got >= n (3?) words\n",
    "    holdings_company_names = holdings[\"company_clean\"].dropna().str.upper().str.strip().unique()\n",
    "    fff[\"company_clean\"] = remove_common_words(fff[fff_company_col], common_words_in_company)\n",
    "    fff[\"company_clean\"] = fff[\"company_clean\"].str.upper().str.strip()\n",
    "    fff_company_names = fff[\"company_clean\"].dropna().unique()\n",
    "    # fuzzy matching company names\n",
    "    print(\"\\n** fuzzy matching company names ** (this could take a few minutes)\")\n",
    "    agg_matches = {}\n",
    "    for c in holdings_company_names:\n",
    "        agg_matches[c] = best_match(c, fff_company_names)\n",
    "    agg_fuzzy_results = pd.DataFrame(agg_matches).transpose()\n",
    "    agg_fuzzy_results.rename({0: 'fff_by_name', 1: 'company_name_match_score'}, axis=1, inplace=True)\n",
    "    agg_fuzzy_results = agg_fuzzy_results[agg_fuzzy_results['company_name_match_score'] > min_match_threshold]\n",
    "    # join back to fff to get fff_fossil_any\n",
    "    fff_company_with_fff_flag_any = fff.groupby('company_clean').first()\n",
    "    agg_fuzzy_results = pd.merge(\n",
    "        left=agg_fuzzy_results,\n",
    "        right=fff_company_with_fff_flag_any[flags + ['fff_flag_any']],\n",
    "        left_on='fff_by_name',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    # add fuzzy match results to holdings\n",
    "    holdings_with_fuzzy = pd.merge(\n",
    "        left=holdings,\n",
    "        right=agg_fuzzy_results,\n",
    "        left_on=\"company_clean\",\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    holdings_with_fuzzy[\"is_\" + agg_flag_name + \"_company_name\"] = holdings_with_fuzzy.apply(\n",
    "        lambda row: row['fff_flag_any'] if row['company_name_match_score'] > flagged_match_threshold else np.nan,\n",
    "        axis='columns'\n",
    "    )\n",
    "    # rename columns\n",
    "    holdings_with_fuzzy = holdings_with_fuzzy.rename({'fff_flag_any': 'fff_by_name_' + agg_flag_name}, axis=1)\n",
    "    # drop redundant columns\n",
    "    if 'company_name_cut_ticker' in holdings_with_fuzzy.columns:\n",
    "        holdings_with_fuzzy = holdings_with_fuzzy.drop(['company_name_cut_ticker'], axis=1)\n",
    "    print(\"Matching by Company Name coverage:\")\n",
    "    print(\"classified: {} out of total holdings: {}\".format(\n",
    "        holdings_with_fuzzy[\"is_\" + agg_flag_name + \"_company_name\"].notnull().sum(),\n",
    "        holdings_with_fuzzy.shape[0]\n",
    "    ))\n",
    "    return holdings_with_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_with_fff_by_company_name = match_holdings_with_fff_by_company_name(\n",
    "    holdings_enriched,\n",
    "    fff,\n",
    "    common_words_in_company=common,\n",
    "    holdings_company_col=holdings_company_col,\n",
    "    flags=flags,\n",
    "    agg_flag_name='dirty',\n",
    "    fff_company_col=\"Company\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check coverage for non-Israeli holdings\n",
    "# check coverage for non-Israeli holdings\n",
    "print(\n",
    "    holdings_with_fff_by_company_name.loc[\n",
    "    ~holdings_with_fff_by_company_name.apply(is_il_holding, axis='columns'),\n",
    "    'is_dirty_company_name'\n",
    "    ].notnull().mean()\n",
    ")\n",
    "\n",
    "holdings_with_fff_by_company_name.loc[\n",
    "    ~holdings_with_fff_by_company_name.apply(is_il_holding, axis='columns'),\n",
    "    'is_dirty_company_name'\n",
    "].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_with_fff_by_company_name['is_dirty_company_name'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_with_fff_by_company_name.loc[\n",
    "    holdings_with_fff_by_company_name[\"is_dirty_company_name\"] == 1,\n",
    "    'שווי'\n",
    "].astype(float).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holdings_with_fff_by_company_name['שווי'].astype(float).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_rename = {\n",
    "    'Deforestation Free Funds: Producer screen': 'Deforestation: Producer',\n",
    "    'Deforestation Free Funds: Financier screen': 'Deforestation: Financier',\n",
    "    'Deforestation Free Funds: Consumer brand screen': 'Deforestation: Consumer brand',\n",
    "    'Gun Free Funds: Gun manufacturers screen': 'Gun manufacturers',\n",
    "    'Gun Free Funds: Gun retailers screen': 'Gun retailers',\n",
    "    'Weapons Free Funds: Major military contractor screen': 'Major military contractor',\n",
    "    'Weapons Free Funds: Cluster munitions / landmines screen': 'Cluster munitions / landmines',\n",
    "    'Weapons Free Funds: Nuclear weapons screen': 'Nuclear weapons',\n",
    "    'Tobacco Free Funds: Tobacco producers screen': 'Tobacco producers'\n",
    "}\n",
    "holdings_with_fff_by_company_name.rename(cols_rename, axis=1, inplace=True)\n",
    "holdings_with_fff_by_company_name.sort_values(\"is_dirty_company_name\", ascending=False).to_excel(\"/Users/urimarom/Downloads/do_no_evil_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE WORKING FROM HERE\n",
    "\n",
    "print(\"\\n8. Calculating is_fossil\")\n",
    "holdings_final = consolidate_is_fossil(holdings_with_fff_by_company_name)\n",
    "# output(holdings_final, \"debug_\" + output_path)\n",
    "# 9. propagate is_fossil across ISIN and LEI (fill in missing is_fossil according to existing ones within group)\n",
    "print(\"\\n9. Propagating is_fossil across il_sec_num, ISIN and LEI\")\n",
    "holdings_propagate_is_fossil = propagate_is_fossil(holdings_final, holdings_il_sec_num_col)\n",
    "holdings_propagate_is_fossil = propagate_is_fossil(holdings_propagate_is_fossil, \"ISIN\")\n",
    "holdings_propagate_is_fossil = propagate_is_fossil(holdings_propagate_is_fossil, \"LEI\")\n",
    "holdings_propagate_is_fossil = add_is_fossil_conflict(holdings_propagate_is_fossil)\n",
    "# output path = input path with 'with fossil classification' added\n",
    "output_path = ''.join(holdings_path.split('.')[:-1]) + ' with fossil classification.' + holdings_path.split('.')[-1]\n",
    "output(holdings_propagate_is_fossil, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich company level data before matching\n",
    "Try to add identifiers that can be used for matching to holdings rather than name matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# mapping - get the most generalized ID (shareClassFIGI? doesn't always exist... name?!)\n",
    "# TODO: try using openFIGI as name normalizer - get to figi_name from both ends\n",
    "def figi_mapping_api_call(id_type, id_value):\n",
    "    api_url = 'https://api.openfigi.com/v3/mapping'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'X-OPENFIGI-APIKEY': 'be5badc9-ca32-495a-b4f5-226da836816c'\n",
    "    }\n",
    "    search = {\n",
    "        \"idType\":id_type,\n",
    "        \"idValue\":id_value\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=[search])\n",
    "        # keep below 25 queries per 6 seconds\n",
    "        time.sleep(240/1000)\n",
    "        if response.status_code == 200 and 'data' in response.json()[0]:\n",
    "            return pd.json_normalize(response.json()[0],'data')\n",
    "        else:\n",
    "            print('Error mapping {}'.format(id_value), response.status_code)\n",
    "            return None\n",
    "    except:\n",
    "        print(\"*Error during mapping*\")\n",
    "        return None\n",
    "\n",
    "def figi_search_api_call(search_term, raw=False):\n",
    "    api_url = 'https://api.openfigi.com/v3/search'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'X-OPENFIGI-APIKEY': 'be5badc9-ca32-495a-b4f5-226da836816c'\n",
    "    }\n",
    "    search = {\n",
    "        \"query\": search_term\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=search)\n",
    "        # keep below 25 queries per 6 seconds\n",
    "        time.sleep(3)\n",
    "        # return response if it's valid and not empty\n",
    "        if response.status_code == 200 and 'data' in response.json():\n",
    "            # raw=True mode for debugging\n",
    "            if raw:\n",
    "                return response.json()\n",
    "            elif response.json()['data']:\n",
    "                return pd.json_normalize(response.json(),'data')\n",
    "            else:\n",
    "                print('No results for {}'.format(search_term), response.status_code)\n",
    "                return None\n",
    "        else:\n",
    "            print('Error searching {}'.format(search_term), response.status_code)\n",
    "            return None\n",
    "    except:\n",
    "        print(\"*Error during search*\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def find_most_similar_string(target, string_list):\n",
    "    similarity_scores = []\n",
    "\n",
    "    for string in string_list:\n",
    "        score = fuzz.ratio(target, string)\n",
    "        similarity_scores.append(score)\n",
    "\n",
    "    max_score = max(similarity_scores)\n",
    "    max_index = similarity_scores.index(max_score)\n",
    "    most_similar_string = string_list[max_index]\n",
    "\n",
    "    return most_similar_string\n",
    "\n",
    "def add_figi_name_to_row_by_ISIN(row):\n",
    "    if row['ISIN']:\n",
    "        figi = figi_mapping_api_call('ID_ISIN', row['ISIN'])\n",
    "        # return None if no results from figi\n",
    "        if figi is None:\n",
    "            return None\n",
    "        else:\n",
    "        # taking the first returned name\n",
    "            name = figi.iloc[0]['name']\n",
    "            return name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_figi_names_to_row_by_ticker(row):\n",
    "    if row['ticker']:\n",
    "        figi = figi_mapping_api_call('TICKER', row['ticker'])\n",
    "        # return None if no results from figi\n",
    "        if figi is None:\n",
    "            return None\n",
    "        else:\n",
    "        # return all names\n",
    "            names = figi['name'].unique()\n",
    "            return names\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_figi_names_to_row_by_name(row):\n",
    "    if row['name']:\n",
    "        figi = figi_search_api_call(row['name'])\n",
    "        # return None if no results from figi\n",
    "        if figi is None:\n",
    "            return None\n",
    "        else:\n",
    "            return figi['name'].unique()\n",
    "        # taking the most similar name\n",
    "#             names = figi['name']\n",
    "#             most_similar_name = find_most_similar_string(row['name'], names)\n",
    "#             return most_similar_name\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_string = \"apple\"\n",
    "string_list = [\"Apple d\", \"banana\", \"Apple d\", \"oranges\"]\n",
    "\n",
    "find_most_similar_string(target_string, string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ISINs = pd.DataFrame(holdings_enriched['ISIN'].unique(), columns = ['ISIN'])\n",
    "all_ISINs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timed apply\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "all_ISINs['FIGI_company_name'] = all_ISINs.progress_apply(add_figi_name_to_row_by_ISIN, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(\n",
    "    left=holdings_enriched,\n",
    "    right=all_ISINs,\n",
    "    on='ISIN',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "result_df.to_csv(\"holdings_enriched_with_figi_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"holdings with FIGI company name: {:,.2f}%\".format(100 * result_df['FIGI_company_name'].notnull().mean()))\n",
    "print(\"holdings missing FIGI company name: {}\".format(result_df['FIGI_company_name'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back to holdings\n",
    "\n",
    "# write function that adds figi names to df\n",
    "def add_figi_name_to_holdings_by_id_type(df, id_type, id_col_name):\n",
    "    \"\"\"Add openFIGI company name to a dataframe using a selected id_type that is stored in id_col_name\n",
    "    :param df: DataFrame with id_col_name\n",
    "    :param id_type: the id_type to be used, one of those: https://www.openfigi.com/api#v3-idType-values\n",
    "    :return: df with FIGI_company_name\n",
    "    \"\"\"\n",
    "    if id_col_name not in df.columns:\n",
    "        return 'missing '+ id_col_name +' column'\n",
    "    else:\n",
    "        uniques = pd.DataFrame(df[id_col_name].unique(), columns = [id_col_name])\n",
    "        if id_type == 'ISIN':\n",
    "            uniques['FIGI_company_name'] = uniques.progress_apply(add_figi_name_to_row_by_ISIN, axis='columns')\n",
    "        else:\n",
    "            print('id_type {} not supported yet'.format(id_type))\n",
    "        result_df = pd.merge(\n",
    "            left=df,\n",
    "            right=uniques,\n",
    "            left_on=id_col_name,\n",
    "            right_on=id_type,\n",
    "            how='left'\n",
    "        )\n",
    "        print(\"holdings with FIGI company name: {:,.2f}%\".format(100 * result_df['FIGI_company_name'].notnull().mean()))\n",
    "        print(\"holdings missing FIGI company name: {}\".format(result_df['FIGI_company_name'].isnull().sum()))\n",
    "        return result_df\n",
    "\n",
    "def get_figi_names_by_id_type(df, id_type, id_col_name):\n",
    "    if id_col_name not in df.columns:\n",
    "        return 'missing '+ id_col_name +' column'\n",
    "    else:\n",
    "        uniques = df[id_col_name].unique()\n",
    "        matches = {}\n",
    "        if id_type == 'name':\n",
    "            for u in uniques:\n",
    "                print(\"matching {} by {}\".format(u, id_type))\n",
    "                m = figi_search_api_call(u)\n",
    "                if m is not None:\n",
    "                    matches[u] = m['name'].unique()\n",
    "        elif id_type == 'TICKER':\n",
    "            for u in uniques:\n",
    "                print(\"matching {} by {}\".format(u, id_type))\n",
    "                mapping = figi_mapping_api_call('TICKER', u)\n",
    "                if mapping is not None:\n",
    "                    matches[u] = mapping['name'].unique()\n",
    "        else:\n",
    "            print('id_type {} not supported yet').format(id_type)\n",
    "            return None\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings_enriched_with_figi = add_figi_name_to_holdings_by_id_type(holdings_enriched, id_type=\"ISIN\", id_col_name=\"ISIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enrich FFF lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff_all = fetch_latest_fff_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle flags - turn into 0/1\n",
    "id_cols = [\"Company\", \"Country\", \"Tickers\"]\n",
    "flag_cols = [c for c in fff_all.columns if c not in id_cols]\n",
    "fff_all[flag_cols] = fff_all[flag_cols].applymap(lambda x: 1 if x == 'Y' else 0)\n",
    "\n",
    "fff_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff_row_per_ticker = fff_all.assign(\n",
    "    Tickers=fff_all['Tickers'].str.split(',')\n",
    ").explode('Tickers')[id_cols].dropna(axis = 0, how = 'all')\n",
    "\n",
    "# look for duplicates\n",
    "fff_row_per_ticker = fff_row_per_ticker[fff_row_per_ticker[\"Tickers\"].notnull()]\n",
    "fff_row_per_ticker[fff_row_per_ticker[\"Tickers\"].duplicated(keep=False)].sort_values(\"Tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple duplicates -> tickers are not a unique identifier (not even with country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT: \n",
    "1. do the same for FFF lists (using Ticker? maybe all tickers and get the name with max matches?)\n",
    "1. check coverage of figi_name in holdings - ISINs with no match still need to be handled somehow\n",
    "1. replace name match! use name match only for residues.\n",
    "1. this can be used to cover other holding_types with ISIN or other IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_response = figi_mapping_api_call('TICKER', 'MMM')\n",
    "ticker_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying search instead\n",
    "\n",
    "(maybe try to combine name and ticker?)\n",
    "for each FFF name, get matching names by name search and matching names by tickers.\n",
    "Take the \"best\" name (most common in all searches?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figi_search_api_call(\"Western Midstream Partners LP\").iloc[0][\"name\"]\n",
    "# figi_search_api_call(\"AT&S Austria Technologie & Systemtechnik AG\").iloc[0][\"name\"]\n",
    "# figi_search_api_call(\"3A-BestGroup JSC\")\n",
    "# figi_search_api_call(\"3M Co\") # gets the wrong one, decapitalized... leaving for now\n",
    "r = figi_search_api_call(\"A. P. Moller Maersk A/S\")\n",
    "df = pd.DataFrame([1])\n",
    "df[\"figi_name\"] = [r['name'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff_all.rename({\"Company\": \"name\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fff_with_figi_by_name = get_figi_names_by_id_type(fff_all, id_type=\"name\", id_col_name=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff_with_figi_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('fff_with_figi_by_name.pkl', 'wb') as handle:\n",
    "    pickle.dump(fff_with_figi_by_name, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('fff_with_figi_by_name.pkl', 'rb') as handle:\n",
    "#     from_pickle = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figi_mapping_api_call('TICKER', '0E6Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff_with_figi_by_ticker = get_figi_names_by_id_type(fff_row_per_ticker, id_type=\"TICKER\", id_col_name=\"Tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fff_with_figi_by_ticker.pkl', 'wb') as handle:\n",
    "    pickle.dump(fff_with_figi_by_ticker, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('fff_with_figi_by_name.pkl', 'rb') as handle:\n",
    "#     from_pickle = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to match each holding to fff by name and ticker.\n",
    "continue working on FFF:\n",
    "1. If no ticker - only name (by best name match with fff_name?)\n",
    "1. If has ticker - keep names that are in both name and ticker match\n",
    "\n",
    "FFF name | Figi names (by ticker and name match)\n",
    "\n",
    "* Join holdings to FFF by name\n",
    "* Name match for holdings with no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "import requests\n",
    "\n",
    "def match_company_names_with_lei_isin(ids, id_type):\n",
    "    api_url = 'https://api.openfigi.com/v3/mapping'\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    mappings = []\n",
    "\n",
    "    for i in ids:\n",
    "        mapping = {\n",
    "            'idType': id_type,\n",
    "            'idValue': i\n",
    "        }\n",
    "        mappings.append(mapping)\n",
    "\n",
    "    request_data = {\n",
    "        'mappings': mappings\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=request_data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        matched_companies = []\n",
    "        for result in results:\n",
    "            if 'data' in result:\n",
    "                for data in result['data']:\n",
    "                    if 'lei' in data and 'isin' in data:\n",
    "                        matched_company = {\n",
    "                            'company_name': data['name'],\n",
    "                            'lei': data['lei'],\n",
    "                            'isin': data['isin']\n",
    "                        }\n",
    "                        matched_companies.append(matched_company)\n",
    "        return matched_companies\n",
    "    else:\n",
    "        print('Error:', response.status_code)\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "company_names = ['Apple Inc', 'Microsoft Corporation', 'Google LLC']\n",
    "matched_results = match_company_names_with_lei_isin(company_names)\n",
    "\n",
    "if matched_results:\n",
    "    for result in matched_results:\n",
    "        print('Company:', result['company_name'])\n",
    "        print('LEI:', result['lei'])\n",
    "        print('ISIN:', result['isin'])\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff_with_figi_by_ticker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
